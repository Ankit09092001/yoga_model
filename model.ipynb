{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "Interpreter = tf.lite.Interpreter\n",
    "import enum\n",
    "from typing import Dict, List, NamedTuple\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BodyPart(enum.Enum):\n",
    "  NOSE = 0\n",
    "  LEFT_EYE = 1\n",
    "  RIGHT_EYE = 2\n",
    "  LEFT_EAR = 3\n",
    "  RIGHT_EAR = 4\n",
    "  LEFT_SHOULDER = 5\n",
    "  RIGHT_SHOULDER = 6\n",
    "  LEFT_ELBOW = 7\n",
    "  RIGHT_ELBOW = 8\n",
    "  LEFT_WRIST = 9\n",
    "  RIGHT_WRIST = 10\n",
    "  LEFT_HIP = 11\n",
    "  RIGHT_HIP = 12\n",
    "  LEFT_KNEE = 13\n",
    "  RIGHT_KNEE = 14\n",
    "  LEFT_ANKLE = 15\n",
    "  RIGHT_ANKLE = 16\n",
    "\n",
    "  \n",
    "class Point(NamedTuple):\n",
    "  \"\"\"A point in 2D space.\"\"\"\n",
    "  x: float\n",
    "  y: float\n",
    "\n",
    "\n",
    "class Rectangle(NamedTuple):\n",
    "  \"\"\"A rectangle in 2D space.\"\"\"\n",
    "  start_point: Point\n",
    "  end_point: Point\n",
    "\n",
    "\n",
    "class KeyPoint(NamedTuple):\n",
    "  \"\"\"A detected human keypoint.\"\"\"\n",
    "  body_part: BodyPart\n",
    "  coordinate: Point\n",
    "  score: float\n",
    "\n",
    "\n",
    "class Person(NamedTuple):\n",
    "  \"\"\"A pose detected by a pose estimation model.\"\"\"\n",
    "  keypoints: List[KeyPoint]\n",
    "  bounding_box: Rectangle\n",
    "  score: float\n",
    "  id: int = None\n",
    "\n",
    "\n",
    "def person_from_keypoints_with_scores(\n",
    "    keypoints_with_scores: np.ndarray,\n",
    "    image_height: float,\n",
    "    image_width: float,\n",
    "    keypoint_score_threshold: float = 0.1) -> Person:\n",
    "\n",
    "  kpts_x = keypoints_with_scores[:, 1]\n",
    "  kpts_y = keypoints_with_scores[:, 0]\n",
    "  scores = keypoints_with_scores[:, 2]\n",
    "\n",
    "  keypoints = []\n",
    "  for i in range(scores.shape[0]):\n",
    "    keypoints.append(\n",
    "        KeyPoint(\n",
    "            BodyPart(i),\n",
    "            Point(int(kpts_x[i] * image_width), int(kpts_y[i] * image_height)),\n",
    "            scores[i]))\n",
    "\n",
    "  start_point = Point(int(np.amin(kpts_x) * image_width), int(np.amin(kpts_y) * image_height))\n",
    "  end_point = Point(int(np.amax(kpts_x) * image_width), int(np.amax(kpts_y) * image_height))\n",
    "  bounding_box = Rectangle(start_point, end_point)\n",
    "\n",
    "  scores_above_threshold = list(\n",
    "      filter(lambda x: x > keypoint_score_threshold, scores))\n",
    "  person_score = np.average(scores_above_threshold)\n",
    "\n",
    "  return Person(keypoints, bounding_box, person_score)\n",
    "\n",
    "\n",
    "class Category(NamedTuple):\n",
    "  label: str\n",
    "  score: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Movenet(object):\n",
    " \n",
    "  _MIN_CROP_KEYPOINT_SCORE = 0.2\n",
    "  _TORSO_EXPANSION_RATIO = 1.9\n",
    "  _BODY_EXPANSION_RATIO = 1.2\n",
    "\n",
    "  def __init__(self, model_name: str) -> None:\n",
    "\n",
    "    _, ext = os.path.splitext(model_name)\n",
    "    if not ext:\n",
    "      model_name += '.tflite'\n",
    "\n",
    "    interpreter = Interpreter(model_path=model_name, num_threads=4)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    self._input_index = interpreter.get_input_details()[0]['index']\n",
    "    self._output_index = interpreter.get_output_details()[0]['index']\n",
    "\n",
    "    self._input_height = interpreter.get_input_details()[0]['shape'][1]\n",
    "    self._input_width = interpreter.get_input_details()[0]['shape'][2]\n",
    "\n",
    "    self._interpreter = interpreter\n",
    "    self._crop_region = None\n",
    "\n",
    "\n",
    "  def init_crop_region(self, image_height: int, image_width: int) -> Dict[(str, float)]:\n",
    "\n",
    "    if image_width > image_height:\n",
    "      x_min = 0.0\n",
    "      box_width = 1.0\n",
    "      y_min = (image_height / 2 - image_width / 2) / image_height\n",
    "      box_height = image_width / image_height\n",
    "    else:\n",
    "      y_min = 0.0\n",
    "      box_height = 1.0\n",
    "      x_min = (image_width / 2 - image_height / 2) / image_width\n",
    "      box_width = image_height / image_width\n",
    "\n",
    "    return {\n",
    "        'y_min': y_min,\n",
    "        'x_min': x_min,\n",
    "        'y_max': y_min + box_height,\n",
    "        'x_max': x_min + box_width,\n",
    "        'height': box_height,\n",
    "        'width': box_width\n",
    "    }\n",
    "\n",
    "  def _torso_visible(self, keypoints: np.ndarray) -> bool:\n",
    "\n",
    "    left_hip_score = keypoints[BodyPart.LEFT_HIP.value, 2]\n",
    "    right_hip_score = keypoints[BodyPart.RIGHT_HIP.value, 2]\n",
    "    left_shoulder_score = keypoints[BodyPart.LEFT_SHOULDER.value, 2]\n",
    "    right_shoulder_score = keypoints[BodyPart.RIGHT_SHOULDER.value, 2]\n",
    "\n",
    "    left_hip_visible = left_hip_score > Movenet._MIN_CROP_KEYPOINT_SCORE\n",
    "    right_hip_visible = right_hip_score > Movenet._MIN_CROP_KEYPOINT_SCORE\n",
    "    left_shoulder_visible = left_shoulder_score > Movenet._MIN_CROP_KEYPOINT_SCORE\n",
    "    right_shoulder_visible = right_shoulder_score > Movenet._MIN_CROP_KEYPOINT_SCORE\n",
    "\n",
    "    return ((left_hip_visible or right_hip_visible) and (left_shoulder_visible or right_shoulder_visible))\n",
    "\n",
    "\n",
    "  def _determine_torso_and_body_range(self, keypoints: np.ndarray, target_keypoints: Dict[(str, float)], center_y: float, center_x: float) -> List[float]:\n",
    "\n",
    "    torso_joints = [BodyPart.LEFT_SHOULDER, BodyPart.RIGHT_SHOULDER, BodyPart.LEFT_HIP, BodyPart.RIGHT_HIP]\n",
    "\n",
    "    max_torso_yrange = 0.0\n",
    "    max_torso_xrange = 0.0\n",
    "    for joint in torso_joints:\n",
    "      dist_y = abs(center_y - target_keypoints[joint][0])\n",
    "      dist_x = abs(center_x - target_keypoints[joint][1])\n",
    "      if dist_y > max_torso_yrange:\n",
    "        max_torso_yrange = dist_y\n",
    "      if dist_x > max_torso_xrange:\n",
    "        max_torso_xrange = dist_x\n",
    "\n",
    "    max_body_yrange = 0.0\n",
    "    max_body_xrange = 0.0\n",
    "    for idx in range(len(BodyPart)):\n",
    "      if keypoints[BodyPart(idx).value, 2] < Movenet._MIN_CROP_KEYPOINT_SCORE:\n",
    "        continue\n",
    "      dist_y = abs(center_y - target_keypoints[joint][0])\n",
    "      dist_x = abs(center_x - target_keypoints[joint][1])\n",
    "      if dist_y > max_body_yrange:\n",
    "        max_body_yrange = dist_y\n",
    "\n",
    "      if dist_x > max_body_xrange:\n",
    "        max_body_xrange = dist_x\n",
    "\n",
    "    return [\n",
    "        max_torso_yrange, max_torso_xrange, max_body_yrange, max_body_xrange\n",
    "    ]\n",
    "\n",
    "\n",
    "  def _determine_crop_region(self, keypoints: np.ndarray, image_height: int, image_width: int) -> Dict[(str, float)]:\n",
    "\n",
    "    target_keypoints = {}\n",
    "    for idx in range(len(BodyPart)):\n",
    "      target_keypoints[BodyPart(idx)] = [keypoints[idx, 0] * image_height, keypoints[idx, 1] * image_width]\n",
    "\n",
    "    if self._torso_visible(keypoints):\n",
    "      center_y = (target_keypoints[BodyPart.LEFT_HIP][0] +target_keypoints[BodyPart.RIGHT_HIP][0]) / 2\n",
    "      center_x = (target_keypoints[BodyPart.LEFT_HIP][1] +target_keypoints[BodyPart.RIGHT_HIP][1]) / 2\n",
    "\n",
    "      (max_torso_yrange, max_torso_xrange, max_body_yrange,max_body_xrange) = self._determine_torso_and_body_range(keypoints, target_keypoints, center_y, center_x)\n",
    "\n",
    "      crop_length_half = np.amax([\n",
    "          max_torso_xrange * Movenet._TORSO_EXPANSION_RATIO,\n",
    "          max_torso_yrange * Movenet._TORSO_EXPANSION_RATIO,\n",
    "          max_body_yrange * Movenet._BODY_EXPANSION_RATIO,\n",
    "          max_body_xrange * Movenet._BODY_EXPANSION_RATIO\n",
    "      ])\n",
    "\n",
    "      distances_to_border = np.array([center_x, image_width - center_x, center_y, image_height - center_y]) \n",
    "      crop_length_half = np.amin([crop_length_half, np.amax(distances_to_border)])\n",
    "\n",
    "      if crop_length_half > max(image_width, image_height) / 2:\n",
    "        return self.init_crop_region(image_height, image_width)\n",
    "      else:\n",
    "        crop_length = crop_length_half * 2\n",
    "      crop_corner = [center_y - crop_length_half, center_x - crop_length_half]\n",
    "      return {\n",
    "          'y_min': crop_corner[0] / image_height,\n",
    "          'x_min': crop_corner[1] / image_width,\n",
    "          'y_max': (crop_corner[0] + crop_length) / image_height,\n",
    "          'x_max': (crop_corner[1] + crop_length) / image_width,\n",
    "          'height': (crop_corner[0] + crop_length) / image_height -crop_corner[0] / image_height,\n",
    "          'width': (crop_corner[1] + crop_length) / image_width -crop_corner[1] / image_width\n",
    "      }\n",
    "\n",
    "    else:\n",
    "      return self.init_crop_region(image_height, image_width)\n",
    "\n",
    "  def _crop_and_resize(self, image: np.ndarray, crop_region: Dict[(str, float)], crop_size ) -> np.ndarray:\n",
    "    y_min, x_min, y_max, x_max = [\n",
    "        crop_region['y_min'], crop_region['x_min'], crop_region['y_max'],\n",
    "        crop_region['x_max']\n",
    "    ]\n",
    "\n",
    "    crop_top = int(0 if y_min < 0 else y_min * image.shape[0])\n",
    "    crop_bottom = int(image.shape[0] if y_max >= 1 else y_max * image.shape[0])\n",
    "    crop_left = int(0 if x_min < 0 else x_min * image.shape[1])\n",
    "    crop_right = int(image.shape[1] if x_max >= 1 else x_max * image.shape[1])\n",
    "\n",
    "    padding_top = int(0 - y_min * image.shape[0] if y_min < 0 else 0)\n",
    "    padding_bottom = int((y_max - 1) * image.shape[0] if y_max >= 1 else 0)\n",
    "    padding_left = int(0 - x_min * image.shape[1] if x_min < 0 else 0)\n",
    "    padding_right = int((x_max - 1) * image.shape[1] if x_max >= 1 else 0)\n",
    "\n",
    "    output_image = image[crop_top:crop_bottom, crop_left:crop_right]\n",
    "    output_image = cv2.copyMakeBorder(output_image, padding_top, padding_bottom, padding_left, padding_right, cv2.BORDER_CONSTANT)\n",
    "    output_image = cv2.resize(output_image, (crop_size[0], crop_size[1]))\n",
    "\n",
    "    return output_image\n",
    "\n",
    "  def _run_detector(self, image: np.ndarray, crop_region: Dict[(str, float)], crop_size ) -> np.ndarray:\n",
    "\n",
    "    input_image = self._crop_and_resize(image, crop_region, crop_size=crop_size)\n",
    "    input_image = input_image.astype(dtype=np.uint8)\n",
    "\n",
    "    self._interpreter.set_tensor(self._input_index, np.expand_dims(input_image, axis=0))\n",
    "    self._interpreter.invoke()\n",
    "    global keypoints_with_scores\n",
    "    keypoints_with_scores = self._interpreter.get_tensor(self._output_index)\n",
    "    keypoints_with_scores = np.squeeze(keypoints_with_scores)\n",
    "\n",
    "    for idx in range(len(BodyPart)):\n",
    "      keypoints_with_scores[idx, 0] = crop_region['y_min'] + crop_region['height'] * keypoints_with_scores[idx, 0]\n",
    "      keypoints_with_scores[idx, 1] = crop_region['x_min'] + crop_region['width'] * keypoints_with_scores[idx, 1]\n",
    "\n",
    "    return keypoints_with_scores\n",
    "\n",
    "  def detect(self, input_image: np.ndarray, reset_crop_region: bool = False) -> Person:\n",
    "\n",
    "    image_height, image_width, _ = input_image.shape\n",
    "    if (self._crop_region is None) or reset_crop_region:\n",
    "      self._crop_region = self.init_crop_region(image_height, image_width)\n",
    "\n",
    "    keypoint_with_scores = self._run_detector(\n",
    "        input_image,\n",
    "        self._crop_region,\n",
    "        crop_size=(self._input_height, self._input_width))\n",
    "    self._crop_region = self._determine_crop_region(keypoint_with_scores, image_height, image_width)\n",
    "\n",
    "    return person_from_keypoints_with_scores(keypoint_with_scores, image_height, image_width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]C:\\Users\\Ankit\\AppData\\Local\\Temp\\ipykernel_4188\\2073420900.py:90: DeprecationWarning: `np.str` is a deprecated alias for the builtin `str`. To silence this warning, use `str` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.str_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  coord = pose_landmarks.flatten().astype(np.str).tolist()\n",
      "100%|██████████| 200/200 [00:16<00:00, 12.39it/s]\n",
      "100%|██████████| 200/200 [00:13<00:00, 14.62it/s]\n",
      "100%|██████████| 200/200 [00:13<00:00, 15.08it/s]\n",
      "100%|██████████| 200/200 [00:13<00:00, 15.28it/s]\n",
      "100%|██████████| 200/200 [00:14<00:00, 14.23it/s]\n",
      "100%|██████████| 84/84 [00:07<00:00, 11.22it/s]\n",
      "100%|██████████| 116/116 [00:08<00:00, 14.12it/s]\n",
      "100%|██████████| 90/90 [00:07<00:00, 11.79it/s]\n",
      "100%|██████████| 96/96 [00:06<00:00, 14.58it/s]\n",
      "100%|██████████| 109/109 [00:07<00:00, 14.59it/s]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import csv\n",
    "import tqdm \n",
    "import wget\n",
    "\n",
    "if('movenet_thunder.tflite' not in os.listdir()):\n",
    "    wget.download('https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/float16/4?lite-format=tflite', 'movenet_thunder.tflite')\n",
    "\n",
    "movenet = Movenet('movenet_thunder')\n",
    "\n",
    "def detect(input_tensor, inference_count=3):\n",
    "    movenet.detect(input_tensor.numpy(), reset_crop_region=True)\n",
    "    \n",
    "    for _ in range(inference_count - 1):\n",
    "        detection = movenet.detect(input_tensor.numpy(), \n",
    "                                reset_crop_region=False)\n",
    "    \n",
    "    return detection\n",
    "\n",
    "class Preprocessor(object):\n",
    "\n",
    "        def __init__(self, images_in_folder,\n",
    "                    csvs_out_path):\n",
    "            self._images_in_folder = images_in_folder\n",
    "            self._csvs_out_path = csvs_out_path\n",
    "            self._csvs_out_folder_per_class = 'csv_per_pose'\n",
    "            \n",
    "            if(self._csvs_out_folder_per_class not in os.listdir()):\n",
    "                os.makedirs(self._csvs_out_folder_per_class)\n",
    "            \n",
    "\n",
    "            self._pose_class_names = sorted(\n",
    "                [n for n in os.listdir(images_in_folder)]\n",
    "            )\n",
    "    \n",
    "\n",
    "        \n",
    "        def process(self, detection_threshold=0.1):\n",
    "\n",
    "            for pose_class_name in self._pose_class_names:\n",
    "\n",
    "                images_in_folder = os.path.join(self._images_in_folder, pose_class_name)\n",
    "                csv_out_path = os.path.join(self._csvs_out_folder_per_class,\n",
    "                                               pose_class_name + '.csv'\n",
    "                                           )\n",
    "\n",
    "                with open(csv_out_path, 'w') as csv_out_file:\n",
    "                    csv_out_writer = csv.writer(csv_out_file,\n",
    "                                                delimiter=',',\n",
    "                                                quoting=csv.QUOTE_MINIMAL\n",
    "                                               )\n",
    "    \n",
    "                    image_names = sorted(\n",
    "                        [n for n in os.listdir(images_in_folder)]\n",
    "                    )\n",
    "                    valid_image_count = 0\n",
    "              \n",
    "                    for image_name in tqdm.tqdm(image_names):\n",
    "                        image_path = os.path.join(images_in_folder, image_name)\n",
    "                        \n",
    "                        try:\n",
    "                            image = tf.io.read_file(image_path)\n",
    "                            image = tf.io.decode_jpeg(image)\n",
    "                        except:\n",
    "                            continue\n",
    "                        \n",
    "                       \n",
    "                        if image.shape[2] != 3:\n",
    "                            continue\n",
    "                        \n",
    "                        person = detect(image)\n",
    "                        \n",
    "                        \n",
    "                        min_landmark_score = min([keypoint.score for keypoint in person.keypoints])\n",
    "                        should_keep_image = min_landmark_score >= detection_threshold\n",
    "                        if not should_keep_image:\n",
    "                            continue\n",
    "                            \n",
    "                        valid_image_count += 1\n",
    "                        \n",
    "                     \n",
    "                        pose_landmarks = np.array(\n",
    "                              [[keypoint.coordinate.x, keypoint.coordinate.y, keypoint.score]\n",
    "                                for keypoint in person.keypoints],\n",
    "                                  dtype=np.float32)\n",
    "                        \n",
    "                        coord = pose_landmarks.flatten().astype(np.str).tolist()\n",
    "                        from warnings import filterwarnings\n",
    "                        filterwarnings(action='ignore', category=DeprecationWarning, message='`np.str` is a deprecated alias')\n",
    "                        csv_out_writer.writerow([image_name] + coord)\n",
    "                        \n",
    "\n",
    "            all_landmarks_df = self.all_landmarks_as_dataframe()\n",
    "            all_landmarks_df.to_csv(self._csvs_out_path, index=False)\n",
    "\n",
    "        def class_names(self):\n",
    "            return self.pose_class_names\n",
    "        \n",
    "        def all_landmarks_as_dataframe(self):\n",
    "\n",
    "            total_df = None\n",
    "            for class_index, class_name in enumerate(self._pose_class_names):\n",
    "                csv_out_path = os.path.join(self._csvs_out_folder_per_class,\n",
    "                                               class_name + '.csv'\n",
    "                                           )\n",
    "                per_class_df = pd.read_csv(csv_out_path, header=None)\n",
    "                \n",
    "                per_class_df['class_no'] = [class_index]*len(per_class_df)\n",
    "                per_class_df['class_name'] = [class_name]*len(per_class_df)\n",
    "                \n",
    "                per_class_df[per_class_df.columns[0]] = class_name + '/' +  per_class_df[per_class_df.columns[0]]\n",
    "                \n",
    "                if total_df is None:\n",
    "                    total_df = per_class_df\n",
    "                else:\n",
    "                    total_df = pd.concat([total_df, per_class_df], axis=0)\n",
    "            \n",
    "            list_name = [[bodypart.name + '_x', bodypart.name + '_y', \n",
    "                  bodypart.name + '_score'] for bodypart in BodyPart]\n",
    "            \n",
    "            header_name = []\n",
    "            for columns_name in list_name:\n",
    "                header_name += columns_name\n",
    "            header_name = ['filename'] + header_name\n",
    "            header_map = { total_df.columns[i]: header_name[i]\n",
    "                             for i in range(len(header_name))\n",
    "                         }\n",
    "            \n",
    "            total_df.rename(header_map, axis=1, inplace=True)\n",
    "            \n",
    "            return total_df\n",
    "\n",
    "\n",
    "images_in_folder = os.path.join('yoga_poses', 'train')\n",
    "csvs_out_path = 'train_data.csv'\n",
    "train_preprocessor = Preprocessor(\n",
    "    images_in_folder,\n",
    "    csvs_out_path\n",
    ")\n",
    "train_preprocessor.process()   \n",
    "\n",
    "\n",
    "images_in_folder = os.path.join('yoga_poses', 'test')\n",
    "csvs_out_path = 'test_data.csv'\n",
    "test_preprocessor = Preprocessor(\n",
    "    images_in_folder,\n",
    "    csvs_out_path\n",
    ")\n",
    "test_preprocessor.process()\n",
    "            \n",
    "            \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------TRAINING----------------\n",
      "Epoch 1/100\n",
      " 1/37 [..............................] - ETA: 9s - loss: 1.7188 - accuracy: 0.0625\n",
      "Epoch 1: val_accuracy improved from -inf to 0.63725, saving model to weights.best.hdf5\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 1.4479 - accuracy: 0.4429 - val_loss: 1.2455 - val_accuracy: 0.6373\n",
      "Epoch 2/100\n",
      " 1/37 [..............................] - ETA: 0s - loss: 1.2252 - accuracy: 0.5625\n",
      "Epoch 2: val_accuracy improved from 0.63725 to 0.66667, saving model to weights.best.hdf5\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1.1028 - accuracy: 0.5779 - val_loss: 0.9490 - val_accuracy: 0.6667\n",
      "Epoch 3/100\n",
      " 1/37 [..............................] - ETA: 0s - loss: 1.1145 - accuracy: 0.5625\n",
      "Epoch 3: val_accuracy improved from 0.66667 to 0.76471, saving model to weights.best.hdf5\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.8846 - accuracy: 0.6332 - val_loss: 0.7673 - val_accuracy: 0.7647\n",
      "Epoch 4/100\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.5766 - accuracy: 0.8125\n",
      "Epoch 4: val_accuracy improved from 0.76471 to 0.81373, saving model to weights.best.hdf5\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.7542 - accuracy: 0.7215 - val_loss: 0.6250 - val_accuracy: 0.8137\n",
      "Epoch 5/100\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.8757 - accuracy: 0.5625\n",
      "Epoch 5: val_accuracy improved from 0.81373 to 0.86275, saving model to weights.best.hdf5\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.6607 - accuracy: 0.7474 - val_loss: 0.5232 - val_accuracy: 0.8627\n",
      "Epoch 6/100\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.7228 - accuracy: 0.6875\n",
      "Epoch 6: val_accuracy improved from 0.86275 to 0.92157, saving model to weights.best.hdf5\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.5894 - accuracy: 0.7855 - val_loss: 0.4325 - val_accuracy: 0.9216\n",
      "Epoch 7/100\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.6547 - accuracy: 0.5625\n",
      "Epoch 7: val_accuracy did not improve from 0.92157\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.4954 - accuracy: 0.8253 - val_loss: 0.3738 - val_accuracy: 0.9216\n",
      "Epoch 8/100\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.5359 - accuracy: 0.7500\n",
      "Epoch 8: val_accuracy improved from 0.92157 to 0.95098, saving model to weights.best.hdf5\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.4445 - accuracy: 0.8512 - val_loss: 0.3141 - val_accuracy: 0.9510\n",
      "Epoch 9/100\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.4162 - accuracy: 0.8125\n",
      "Epoch 9: val_accuracy did not improve from 0.95098\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.4093 - accuracy: 0.8512 - val_loss: 0.2715 - val_accuracy: 0.9510\n",
      "Epoch 10/100\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.2928 - accuracy: 0.9375\n",
      "Epoch 10: val_accuracy improved from 0.95098 to 0.97059, saving model to weights.best.hdf5\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3532 - accuracy: 0.8858 - val_loss: 0.2228 - val_accuracy: 0.9706\n",
      "Epoch 11/100\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.2951 - accuracy: 0.8750\n",
      "Epoch 11: val_accuracy improved from 0.97059 to 0.98039, saving model to weights.best.hdf5\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.3121 - accuracy: 0.8997 - val_loss: 0.1921 - val_accuracy: 0.9804\n",
      "Epoch 12/100\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.3777 - accuracy: 0.8750\n",
      "Epoch 12: val_accuracy did not improve from 0.98039\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2945 - accuracy: 0.8997 - val_loss: 0.1726 - val_accuracy: 0.9804\n",
      "Epoch 13/100\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.1913 - accuracy: 0.9375\n",
      "Epoch 13: val_accuracy did not improve from 0.98039\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2732 - accuracy: 0.9152 - val_loss: 0.1571 - val_accuracy: 0.9804\n",
      "Epoch 14/100\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.3002 - accuracy: 0.9375\n",
      "Epoch 14: val_accuracy did not improve from 0.98039\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2525 - accuracy: 0.9360 - val_loss: 0.1492 - val_accuracy: 0.9804\n",
      "Epoch 15/100\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.1870 - accuracy: 1.0000\n",
      "Epoch 15: val_accuracy did not improve from 0.98039\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2387 - accuracy: 0.9170 - val_loss: 0.1303 - val_accuracy: 0.9804\n",
      "Epoch 16/100\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.3807 - accuracy: 0.8750\n",
      "Epoch 16: val_accuracy improved from 0.98039 to 0.99020, saving model to weights.best.hdf5\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2216 - accuracy: 0.9239 - val_loss: 0.1110 - val_accuracy: 0.9902\n",
      "Epoch 17/100\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.1034 - accuracy: 1.0000\n",
      "Epoch 17: val_accuracy did not improve from 0.99020\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2041 - accuracy: 0.9412 - val_loss: 0.1027 - val_accuracy: 0.9902\n",
      "Epoch 18/100\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.1611 - accuracy: 0.8750\n",
      "Epoch 18: val_accuracy did not improve from 0.99020\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.1975 - accuracy: 0.9429 - val_loss: 0.0925 - val_accuracy: 0.9902\n",
      "Epoch 19/100\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.0966 - accuracy: 1.0000\n",
      "Epoch 19: val_accuracy did not improve from 0.99020\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.1892 - accuracy: 0.9394 - val_loss: 0.0883 - val_accuracy: 0.9902\n",
      "Epoch 20/100\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.3127 - accuracy: 0.9375\n",
      "Epoch 20: val_accuracy did not improve from 0.99020\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.1935 - accuracy: 0.9291 - val_loss: 0.0836 - val_accuracy: 0.9902\n",
      "Epoch 21/100\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.1958 - accuracy: 0.9375\n",
      "Epoch 21: val_accuracy did not improve from 0.99020\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.1561 - accuracy: 0.9585 - val_loss: 0.0754 - val_accuracy: 0.9902\n",
      "Epoch 22/100\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.1317 - accuracy: 0.9375\n",
      "Epoch 22: val_accuracy did not improve from 0.99020\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.1643 - accuracy: 0.9550 - val_loss: 0.0696 - val_accuracy: 0.9902\n",
      "Epoch 23/100\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.1675 - accuracy: 0.9375\n",
      "Epoch 23: val_accuracy did not improve from 0.99020\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.1505 - accuracy: 0.9567 - val_loss: 0.0674 - val_accuracy: 0.9902\n",
      "Epoch 24/100\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.0680 - accuracy: 0.9375\n",
      "Epoch 24: val_accuracy did not improve from 0.99020\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.1664 - accuracy: 0.9481 - val_loss: 0.0598 - val_accuracy: 0.9902\n",
      "Epoch 25/100\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.0285 - accuracy: 1.0000\n",
      "Epoch 25: val_accuracy did not improve from 0.99020\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.1368 - accuracy: 0.9533 - val_loss: 0.0618 - val_accuracy: 0.9902\n",
      "Epoch 26/100\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.1465 - accuracy: 0.9375\n",
      "Epoch 26: val_accuracy did not improve from 0.99020\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.1464 - accuracy: 0.9637 - val_loss: 0.0570 - val_accuracy: 0.9902\n",
      "Epoch 27/100\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.1197 - accuracy: 1.0000\n",
      "Epoch 27: val_accuracy did not improve from 0.99020\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.1267 - accuracy: 0.9567 - val_loss: 0.0539 - val_accuracy: 0.9902\n",
      "Epoch 28/100\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.0661 - accuracy: 1.0000\n",
      "Epoch 28: val_accuracy did not improve from 0.99020\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.1126 - accuracy: 0.9706 - val_loss: 0.0551 - val_accuracy: 0.9902\n",
      "Epoch 29/100\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.0864 - accuracy: 1.0000\n",
      "Epoch 29: val_accuracy did not improve from 0.99020\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.1181 - accuracy: 0.9550 - val_loss: 0.0574 - val_accuracy: 0.9902\n",
      "Epoch 30/100\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.0703 - accuracy: 1.0000\n",
      "Epoch 30: val_accuracy did not improve from 0.99020\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.1049 - accuracy: 0.9723 - val_loss: 0.0565 - val_accuracy: 0.9902\n",
      "Epoch 31/100\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.0977 - accuracy: 1.0000\n",
      "Epoch 31: val_accuracy did not improve from 0.99020\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.1026 - accuracy: 0.9758 - val_loss: 0.0527 - val_accuracy: 0.9902\n",
      "Epoch 32/100\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.2679 - accuracy: 0.8750\n",
      "Epoch 32: val_accuracy did not improve from 0.99020\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.0928 - accuracy: 0.9723 - val_loss: 0.0550 - val_accuracy: 0.9902\n",
      "Epoch 33/100\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.1810 - accuracy: 0.8750\n",
      "Epoch 33: val_accuracy did not improve from 0.99020\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.0945 - accuracy: 0.9706 - val_loss: 0.0487 - val_accuracy: 0.9902\n",
      "Epoch 34/100\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.1315 - accuracy: 0.9375\n",
      "Epoch 34: val_accuracy did not improve from 0.99020\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.0919 - accuracy: 0.9792 - val_loss: 0.0504 - val_accuracy: 0.9902\n",
      "Epoch 35/100\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.0710 - accuracy: 1.0000\n",
      "Epoch 35: val_accuracy did not improve from 0.99020\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.0908 - accuracy: 0.9706 - val_loss: 0.0548 - val_accuracy: 0.9902\n",
      "Epoch 36/100\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.0688 - accuracy: 1.0000\n",
      "Epoch 36: val_accuracy did not improve from 0.99020\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.0748 - accuracy: 0.9862 - val_loss: 0.0486 - val_accuracy: 0.9902\n",
      "-----------------EVAUATION----------------\n",
      "14/14 [==============================] - 0s 692us/step - loss: 0.0121 - accuracy: 0.9976\n",
      "LOSS:  0.012124519795179367\n",
      "ACCURACY:  0.9976470470428467\n",
      "tfjs model saved at  model\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflowjs as tfjs\n",
    "\n",
    "tfjs_model_dir = 'model'\n",
    "\n",
    "\n",
    "# loading final csv file\n",
    "def load_csv(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df.drop(['filename'],axis=1, inplace=True)\n",
    "    classes = df.pop('class_name').unique()\n",
    "    y = df.pop('class_no')\n",
    "    \n",
    "    X = df.astype('float64')\n",
    "    y = keras.utils.to_categorical(y)\n",
    "    \n",
    "    return X, y, classes\n",
    "\n",
    "\n",
    "def get_center_point(landmarks, left_bodypart, right_bodypart):\n",
    "    \"\"\"Calculates the center point of the two given landmarks.\"\"\"\n",
    "    left = tf.gather(landmarks, left_bodypart.value, axis=1)\n",
    "    right = tf.gather(landmarks, right_bodypart.value, axis=1)\n",
    "    center = left * 0.5 + right * 0.5\n",
    "    return center\n",
    "\n",
    "\n",
    "def get_pose_size(landmarks, torso_size_multiplier=2.5):\n",
    "    \"\"\"Calculates pose size.\n",
    "\n",
    "    It is the maximum of two values:\n",
    "    * Torso size multiplied by `torso_size_multiplier`\n",
    "    * Maximum distance from pose center to any pose landmark\n",
    "    \"\"\"\n",
    "    # Hips center\n",
    "    hips_center = get_center_point(landmarks, BodyPart.LEFT_HIP, \n",
    "                                 BodyPart.RIGHT_HIP)\n",
    "\n",
    "    # Shoulders center\n",
    "    shoulders_center = get_center_point(landmarks, BodyPart.LEFT_SHOULDER,\n",
    "                                      BodyPart.RIGHT_SHOULDER)\n",
    "\n",
    "    # Torso size as the minimum body size\n",
    "    torso_size = tf.linalg.norm(shoulders_center - hips_center)\n",
    "    # Pose center\n",
    "    pose_center_new = get_center_point(landmarks, BodyPart.LEFT_HIP, \n",
    "                                     BodyPart.RIGHT_HIP)\n",
    "    pose_center_new = tf.expand_dims(pose_center_new, axis=1)\n",
    "    # Broadcast the pose center to the same size as the landmark vector to\n",
    "    # perform substraction\n",
    "    pose_center_new = tf.broadcast_to(pose_center_new,\n",
    "                                    [tf.size(landmarks) // (17*2), 17, 2])\n",
    "\n",
    "    # Dist to pose center\n",
    "    d = tf.gather(landmarks - pose_center_new, 0, axis=0,\n",
    "                name=\"dist_to_pose_center\")\n",
    "    # Max dist to pose center\n",
    "    max_dist = tf.reduce_max(tf.linalg.norm(d, axis=0))\n",
    "\n",
    "    # Normalize scale\n",
    "    pose_size = tf.maximum(torso_size * torso_size_multiplier, max_dist)\n",
    "    return pose_size\n",
    "\n",
    "\n",
    "\n",
    "def normalize_pose_landmarks(landmarks):\n",
    "    \"\"\"Normalizes the landmarks translation by moving the pose center to (0,0) and\n",
    "    scaling it to a constant pose size.\n",
    "  \"\"\"\n",
    "  # Move landmarks so that the pose center becomes (0,0)\n",
    "    pose_center = get_center_point(landmarks, BodyPart.LEFT_HIP, \n",
    "                                 BodyPart.RIGHT_HIP)\n",
    "\n",
    "    pose_center = tf.expand_dims(pose_center, axis=1)\n",
    "    # Broadcast the pose center to the same size as the landmark vector to perform\n",
    "    # substraction\n",
    "    pose_center = tf.broadcast_to(pose_center, \n",
    "                                [tf.size(landmarks) // (17*2), 17, 2])\n",
    "    landmarks = landmarks - pose_center\n",
    "\n",
    "    # Scale the landmarks to a constant pose size\n",
    "    pose_size = get_pose_size(landmarks)\n",
    "    landmarks /= pose_size\n",
    "    return landmarks\n",
    "\n",
    "\n",
    "def landmarks_to_embedding(landmarks_and_scores):\n",
    "    \"\"\"Converts the input landmarks into a pose embedding.\"\"\"\n",
    "    # Reshape the flat input into a matrix with shape=(17, 3)\n",
    "    reshaped_inputs = keras.layers.Reshape((17, 3))(landmarks_and_scores)\n",
    "\n",
    "    # Normalize landmarks 2D\n",
    "    landmarks = normalize_pose_landmarks(reshaped_inputs[:, :, :2])\n",
    "    # Flatten the normalized landmark coordinates into a vector\n",
    "    embedding = keras.layers.Flatten()(landmarks)\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def preprocess_data(X_train):\n",
    "    processed_X_train = []\n",
    "    for i in range(X_train.shape[0]):\n",
    "        embedding = landmarks_to_embedding(tf.reshape(tf.convert_to_tensor(X_train.iloc[i]), (1, 51)))\n",
    "        processed_X_train.append(tf.reshape(embedding, (34)))\n",
    "    return tf.convert_to_tensor(processed_X_train)\n",
    "\n",
    "\n",
    "X, y, class_names = load_csv(\"train_data.csv\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15)\n",
    "X_test, y_test, _ = load_csv('test_data.csv')\n",
    "\n",
    "\n",
    "processed_X_train = preprocess_data(X_train)\n",
    "processed_X_val =  preprocess_data(X_val)\n",
    "processed_X_test = preprocess_data(X_test)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(34))\n",
    "layer = keras.layers.Dense(128, activation=tf.nn.relu6)(inputs)\n",
    "layer = keras.layers.Dropout(0.5)(layer)\n",
    "layer = keras.layers.Dense(64, activation=tf.nn.relu6)(layer)\n",
    "layer = keras.layers.Dropout(0.5)(layer)\n",
    "outputs = keras.layers.Dense(len(class_names), activation=\"softmax\")(layer)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Add a checkpoint callback to store the checkpoint that has the highest\n",
    "# validation accuracy.\n",
    "checkpoint_path = \"weights.best.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                             monitor='val_accuracy',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='max')\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor='val_accuracy', \n",
    "                                              patience=20)\n",
    "\n",
    "# Start training\n",
    "print('--------------TRAINING----------------')\n",
    "history = model.fit(processed_X_train, y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=16,\n",
    "                    validation_data=(processed_X_val, y_val),\n",
    "                    callbacks=[checkpoint, earlystopping])\n",
    "\n",
    "\n",
    "print('-----------------EVAUATION----------------')\n",
    "loss, accuracy = model.evaluate(processed_X_test, y_test)\n",
    "print('LOSS: ', loss)\n",
    "print(\"ACCURACY: \", accuracy)\n",
    "\n",
    "\n",
    "tfjs.converters.save_keras_model(model, tfjs_model_dir)\n",
    "print('tfjs model saved at ',tfjs_model_dir)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
