{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import enum\n",
    "from typing import Dict, List, NamedTuple\n",
    "import pandas as pd \n",
    "import csv\n",
    "import wget\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflowjs as tfjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BodyPart(enum.Enum):\n",
    "  NOSE = 0\n",
    "  LEFT_EYE = 1\n",
    "  RIGHT_EYE = 2\n",
    "  LEFT_EAR = 3\n",
    "  RIGHT_EAR = 4\n",
    "  LEFT_SHOULDER = 5\n",
    "  RIGHT_SHOULDER = 6\n",
    "  LEFT_ELBOW = 7\n",
    "  RIGHT_ELBOW = 8\n",
    "  LEFT_WRIST = 9\n",
    "  RIGHT_WRIST = 10\n",
    "  LEFT_HIP = 11\n",
    "  RIGHT_HIP = 12\n",
    "  LEFT_KNEE = 13\n",
    "  RIGHT_KNEE = 14\n",
    "  LEFT_ANKLE = 15\n",
    "  RIGHT_ANKLE = 16\n",
    "\n",
    "  \n",
    "class Point(NamedTuple):\n",
    "  \"\"\"A point in 2D space.\"\"\"\n",
    "  x: float\n",
    "  y: float\n",
    "\n",
    "\n",
    "class Rectangle(NamedTuple):\n",
    "  \"\"\"A rectangle in 2D space.\"\"\"\n",
    "  start_point: Point\n",
    "  end_point: Point\n",
    "\n",
    "\n",
    "class KeyPoint(NamedTuple):\n",
    "  \"\"\"A detected human keypoint.\"\"\"\n",
    "  body_part: BodyPart\n",
    "  coordinate: Point\n",
    "  score: float\n",
    "\n",
    "\n",
    "class Person(NamedTuple):\n",
    "  \"\"\"A pose detected by a pose estimation model.\"\"\"\n",
    "  keypoints: List[KeyPoint]\n",
    "  bounding_box: Rectangle\n",
    "  score: float\n",
    "  id: int = None\n",
    "\n",
    "\n",
    "def person_from_keypoints_with_scores(\n",
    "    keypoints_with_scores: np.ndarray,\n",
    "    image_height: float,\n",
    "    image_width: float,\n",
    "    keypoint_score_threshold: float = 0.1) -> Person:\n",
    "\n",
    "  kpts_x = keypoints_with_scores[:, 1]\n",
    "  kpts_y = keypoints_with_scores[:, 0]\n",
    "  scores = keypoints_with_scores[:, 2]\n",
    "\n",
    "  keypoints = []\n",
    "  for i in range(scores.shape[0]):\n",
    "    keypoints.append(\n",
    "        KeyPoint(\n",
    "            BodyPart(i),\n",
    "            Point(int(kpts_x[i] * image_width), int(kpts_y[i] * image_height)),\n",
    "            scores[i]))\n",
    "\n",
    "  start_point = Point(int(np.amin(kpts_x) * image_width), int(np.amin(kpts_y) * image_height))\n",
    "  end_point = Point(int(np.amax(kpts_x) * image_width), int(np.amax(kpts_y) * image_height))\n",
    "  bounding_box = Rectangle(start_point, end_point)\n",
    "\n",
    "  scores_above_threshold = list(\n",
    "      filter(lambda x: x > keypoint_score_threshold, scores))\n",
    "  person_score = np.average(scores_above_threshold)\n",
    "\n",
    "  return Person(keypoints, bounding_box, person_score)\n",
    "\n",
    "\n",
    "class Category(NamedTuple):\n",
    "  label: str\n",
    "  score: float\n",
    "\n",
    "\n",
    "Interpreter = tf.lite.Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Movenet(object):\n",
    " \n",
    "  _MIN_CROP_KEYPOINT_SCORE = 0.2\n",
    "  _TORSO_EXPANSION_RATIO = 1.9\n",
    "  _BODY_EXPANSION_RATIO = 1.2\n",
    "\n",
    "  def __init__(self, model_name: str) -> None:\n",
    "\n",
    "    _, ext = os.path.splitext(model_name)\n",
    "    if not ext:\n",
    "      model_name += '.tflite'\n",
    "\n",
    "    interpreter = Interpreter(model_path=model_name, num_threads=4)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    self._input_index = interpreter.get_input_details()[0]['index']\n",
    "    self._output_index = interpreter.get_output_details()[0]['index']\n",
    "\n",
    "    self._input_height = interpreter.get_input_details()[0]['shape'][1]\n",
    "    self._input_width = interpreter.get_input_details()[0]['shape'][2]\n",
    "\n",
    "    self._interpreter = interpreter\n",
    "    self._crop_region = None\n",
    "\n",
    "\n",
    "  def init_crop_region(self, image_height: int, image_width: int) -> Dict[(str, float)]:\n",
    "\n",
    "    if image_width > image_height:\n",
    "      x_min = 0.0\n",
    "      box_width = 1.0\n",
    "      y_min = (image_height / 2 - image_width / 2) / image_height\n",
    "      box_height = image_width / image_height\n",
    "    else:\n",
    "      y_min = 0.0\n",
    "      box_height = 1.0\n",
    "      x_min = (image_width / 2 - image_height / 2) / image_width\n",
    "      box_width = image_height / image_width\n",
    "\n",
    "    return {\n",
    "        'y_min': y_min,\n",
    "        'x_min': x_min,\n",
    "        'y_max': y_min + box_height,\n",
    "        'x_max': x_min + box_width,\n",
    "        'height': box_height,\n",
    "        'width': box_width\n",
    "    }\n",
    "\n",
    "  def _torso_visible(self, keypoints: np.ndarray) -> bool:\n",
    "\n",
    "    left_hip_score = keypoints[BodyPart.LEFT_HIP.value, 2]\n",
    "    right_hip_score = keypoints[BodyPart.RIGHT_HIP.value, 2]\n",
    "    left_shoulder_score = keypoints[BodyPart.LEFT_SHOULDER.value, 2]\n",
    "    right_shoulder_score = keypoints[BodyPart.RIGHT_SHOULDER.value, 2]\n",
    "\n",
    "    left_hip_visible = left_hip_score > Movenet._MIN_CROP_KEYPOINT_SCORE\n",
    "    right_hip_visible = right_hip_score > Movenet._MIN_CROP_KEYPOINT_SCORE\n",
    "    left_shoulder_visible = left_shoulder_score > Movenet._MIN_CROP_KEYPOINT_SCORE\n",
    "    right_shoulder_visible = right_shoulder_score > Movenet._MIN_CROP_KEYPOINT_SCORE\n",
    "\n",
    "    return ((left_hip_visible or right_hip_visible) and (left_shoulder_visible or right_shoulder_visible))\n",
    "\n",
    "\n",
    "  def _determine_torso_and_body_range(self, keypoints: np.ndarray, target_keypoints: Dict[(str, float)], center_y: float, center_x: float) -> List[float]:\n",
    "\n",
    "    torso_joints = [BodyPart.LEFT_SHOULDER, BodyPart.RIGHT_SHOULDER, BodyPart.LEFT_HIP, BodyPart.RIGHT_HIP]\n",
    "\n",
    "    max_torso_yrange = 0.0\n",
    "    max_torso_xrange = 0.0\n",
    "    for joint in torso_joints:\n",
    "      dist_y = abs(center_y - target_keypoints[joint][0])\n",
    "      dist_x = abs(center_x - target_keypoints[joint][1])\n",
    "      if dist_y > max_torso_yrange:\n",
    "        max_torso_yrange = dist_y\n",
    "      if dist_x > max_torso_xrange:\n",
    "        max_torso_xrange = dist_x\n",
    "\n",
    "    max_body_yrange = 0.0\n",
    "    max_body_xrange = 0.0\n",
    "    for idx in range(len(BodyPart)):\n",
    "      if keypoints[BodyPart(idx).value, 2] < Movenet._MIN_CROP_KEYPOINT_SCORE:\n",
    "        continue\n",
    "      dist_y = abs(center_y - target_keypoints[joint][0])\n",
    "      dist_x = abs(center_x - target_keypoints[joint][1])\n",
    "      if dist_y > max_body_yrange:\n",
    "        max_body_yrange = dist_y\n",
    "\n",
    "      if dist_x > max_body_xrange:\n",
    "        max_body_xrange = dist_x\n",
    "\n",
    "    return [\n",
    "        max_torso_yrange, max_torso_xrange, max_body_yrange, max_body_xrange\n",
    "    ]\n",
    "\n",
    "\n",
    "  def _determine_crop_region(self, keypoints: np.ndarray, image_height: int, image_width: int) -> Dict[(str, float)]:\n",
    "\n",
    "    target_keypoints = {}\n",
    "    for idx in range(len(BodyPart)):\n",
    "      target_keypoints[BodyPart(idx)] = [keypoints[idx, 0] * image_height, keypoints[idx, 1] * image_width]\n",
    "\n",
    "    if self._torso_visible(keypoints):\n",
    "      center_y = (target_keypoints[BodyPart.LEFT_HIP][0] +target_keypoints[BodyPart.RIGHT_HIP][0]) / 2\n",
    "      center_x = (target_keypoints[BodyPart.LEFT_HIP][1] +target_keypoints[BodyPart.RIGHT_HIP][1]) / 2\n",
    "\n",
    "      (max_torso_yrange, max_torso_xrange, max_body_yrange,max_body_xrange) = self._determine_torso_and_body_range(keypoints, target_keypoints, center_y, center_x)\n",
    "\n",
    "      crop_length_half = np.amax([\n",
    "          max_torso_xrange * Movenet._TORSO_EXPANSION_RATIO,\n",
    "          max_torso_yrange * Movenet._TORSO_EXPANSION_RATIO,\n",
    "          max_body_yrange * Movenet._BODY_EXPANSION_RATIO,\n",
    "          max_body_xrange * Movenet._BODY_EXPANSION_RATIO\n",
    "      ])\n",
    "\n",
    "      distances_to_border = np.array([center_x, image_width - center_x, center_y, image_height - center_y]) \n",
    "      crop_length_half = np.amin([crop_length_half, np.amax(distances_to_border)])\n",
    "\n",
    "      if crop_length_half > max(image_width, image_height) / 2:\n",
    "        return self.init_crop_region(image_height, image_width)\n",
    "      else:\n",
    "        crop_length = crop_length_half * 2\n",
    "      crop_corner = [center_y - crop_length_half, center_x - crop_length_half]\n",
    "      return {\n",
    "          'y_min': crop_corner[0] / image_height,\n",
    "          'x_min': crop_corner[1] / image_width,\n",
    "          'y_max': (crop_corner[0] + crop_length) / image_height,\n",
    "          'x_max': (crop_corner[1] + crop_length) / image_width,\n",
    "          'height': (crop_corner[0] + crop_length) / image_height -crop_corner[0] / image_height,\n",
    "          'width': (crop_corner[1] + crop_length) / image_width -crop_corner[1] / image_width\n",
    "      }\n",
    "\n",
    "    else:\n",
    "      return self.init_crop_region(image_height, image_width)\n",
    "\n",
    "  def _crop_and_resize(self, image: np.ndarray, crop_region: Dict[(str, float)], crop_size ) -> np.ndarray:\n",
    "    y_min, x_min, y_max, x_max = [\n",
    "        crop_region['y_min'], crop_region['x_min'], crop_region['y_max'],\n",
    "        crop_region['x_max']\n",
    "    ]\n",
    "\n",
    "    crop_top = int(0 if y_min < 0 else y_min * image.shape[0])\n",
    "    crop_bottom = int(image.shape[0] if y_max >= 1 else y_max * image.shape[0])\n",
    "    crop_left = int(0 if x_min < 0 else x_min * image.shape[1])\n",
    "    crop_right = int(image.shape[1] if x_max >= 1 else x_max * image.shape[1])\n",
    "\n",
    "    padding_top = int(0 - y_min * image.shape[0] if y_min < 0 else 0)\n",
    "    padding_bottom = int((y_max - 1) * image.shape[0] if y_max >= 1 else 0)\n",
    "    padding_left = int(0 - x_min * image.shape[1] if x_min < 0 else 0)\n",
    "    padding_right = int((x_max - 1) * image.shape[1] if x_max >= 1 else 0)\n",
    "\n",
    "    output_image = image[crop_top:crop_bottom, crop_left:crop_right]\n",
    "    output_image = cv2.copyMakeBorder(output_image, padding_top, padding_bottom, padding_left, padding_right, cv2.BORDER_CONSTANT)\n",
    "    output_image = cv2.resize(output_image, (crop_size[0], crop_size[1]))\n",
    "\n",
    "    return output_image\n",
    "\n",
    "  def _run_detector(self, image: np.ndarray, crop_region: Dict[(str, float)], crop_size ) -> np.ndarray:\n",
    "\n",
    "    input_image = self._crop_and_resize(image, crop_region, crop_size=crop_size)\n",
    "    input_image = input_image.astype(dtype=np.uint8)\n",
    "\n",
    "    self._interpreter.set_tensor(self._input_index, np.expand_dims(input_image, axis=0))\n",
    "    self._interpreter.invoke()\n",
    "    global keypoints_with_scores\n",
    "    keypoints_with_scores = self._interpreter.get_tensor(self._output_index)\n",
    "    keypoints_with_scores = np.squeeze(keypoints_with_scores)\n",
    "\n",
    "    for idx in range(len(BodyPart)):\n",
    "      keypoints_with_scores[idx, 0] = crop_region['y_min'] + crop_region['height'] * keypoints_with_scores[idx, 0]\n",
    "      keypoints_with_scores[idx, 1] = crop_region['x_min'] + crop_region['width'] * keypoints_with_scores[idx, 1]\n",
    "\n",
    "    return keypoints_with_scores\n",
    "\n",
    "  def detect(self, input_image: np.ndarray, reset_crop_region: bool = False) -> Person:\n",
    "\n",
    "    image_height, image_width, _ = input_image.shape\n",
    "    if (self._crop_region is None) or reset_crop_region:\n",
    "      self._crop_region = self.init_crop_region(image_height, image_width)\n",
    "\n",
    "    keypoint_with_scores = self._run_detector(\n",
    "        input_image,\n",
    "        self._crop_region,\n",
    "        crop_size=(self._input_height, self._input_width))\n",
    "    self._crop_region = self._determine_crop_region(keypoint_with_scores, image_height, image_width)\n",
    "\n",
    "    return person_from_keypoints_with_scores(keypoint_with_scores, image_height, image_width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if('movenet_thunder.tflite' not in os.listdir()):\n",
    "    wget.download('https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/float16/4?lite-format=tflite', 'movenet_thunder.tflite')\n",
    "\n",
    "movenet = Movenet('movenet_thunder')\n",
    "\n",
    "def detect(input_tensor, inference_count=3):\n",
    "    movenet.detect(input_tensor.numpy(), reset_crop_region=True)\n",
    "    \n",
    "    for _ in range(inference_count - 1):\n",
    "        detection = movenet.detect(input_tensor.numpy(), \n",
    "                                reset_crop_region=False)\n",
    "    \n",
    "    return detection\n",
    "\n",
    "class Preprocessor(object):\n",
    "\n",
    "        def __init__(self, images_in_folder,\n",
    "                    csvs_out_path):\n",
    "            self._images_in_folder = images_in_folder\n",
    "            self._csvs_out_path = csvs_out_path\n",
    "            self._csvs_out_folder_per_class = 'csv_per_pose'\n",
    "            \n",
    "            if(self._csvs_out_folder_per_class not in os.listdir()):\n",
    "                os.makedirs(self._csvs_out_folder_per_class)\n",
    "            \n",
    "\n",
    "            self._pose_class_names = sorted(\n",
    "                [n for n in os.listdir(images_in_folder)]\n",
    "            )\n",
    "    \n",
    "\n",
    "        \n",
    "        def process(self, detection_threshold=0.1):\n",
    "\n",
    "            for pose_class_name in self._pose_class_names:\n",
    "\n",
    "                images_in_folder = os.path.join(self._images_in_folder, pose_class_name)\n",
    "                csv_out_path = os.path.join(self._csvs_out_folder_per_class,\n",
    "                                               pose_class_name + '.csv'\n",
    "                                           )\n",
    "\n",
    "                with open(csv_out_path, 'w') as csv_out_file:\n",
    "                    csv_out_writer = csv.writer(csv_out_file,\n",
    "                                                delimiter=',',\n",
    "                                                quoting=csv.QUOTE_MINIMAL\n",
    "                                               )\n",
    "    \n",
    "                    image_names = sorted(\n",
    "                        [n for n in os.listdir(images_in_folder)]\n",
    "                    )\n",
    "                    valid_image_count = 0\n",
    "              \n",
    "                    for image_name in (image_names):\n",
    "                        image_path = os.path.join(images_in_folder, image_name)\n",
    "                        \n",
    "                        try:\n",
    "                            image = tf.io.read_file(image_path)\n",
    "                            image = tf.io.decode_jpeg(image)\n",
    "                        except:\n",
    "                            continue\n",
    "                        \n",
    "                       \n",
    "                        if image.shape[2] != 3:\n",
    "                            continue\n",
    "                        \n",
    "                        person = detect(image)\n",
    "                        \n",
    "                        \n",
    "                        min_landmark_score = min([keypoint.score for keypoint in person.keypoints])\n",
    "                        should_keep_image = min_landmark_score >= detection_threshold\n",
    "                        if not should_keep_image:\n",
    "                            continue\n",
    "                            \n",
    "                        valid_image_count += 1\n",
    "                        \n",
    "                     \n",
    "                        pose_landmarks = np.array(\n",
    "                              [[keypoint.coordinate.x, keypoint.coordinate.y, keypoint.score]\n",
    "                                for keypoint in person.keypoints],\n",
    "                                  dtype=np.float32)\n",
    "                        \n",
    "                        coord = pose_landmarks.flatten().astype(np.str).tolist()\n",
    "                        from warnings import filterwarnings\n",
    "                        filterwarnings(action='ignore', category=DeprecationWarning, message='`np.str` is a deprecated alias')\n",
    "                        csv_out_writer.writerow([image_name] + coord)\n",
    "                        \n",
    "\n",
    "            all_landmarks_df = self.all_landmarks_as_dataframe()\n",
    "            all_landmarks_df.to_csv(self._csvs_out_path, index=False)\n",
    "\n",
    "        def class_names(self):\n",
    "            return self.pose_class_names\n",
    "        \n",
    "        def all_landmarks_as_dataframe(self):\n",
    "\n",
    "            total_df = None\n",
    "            for class_index, class_name in enumerate(self._pose_class_names):\n",
    "                csv_out_path = os.path.join(self._csvs_out_folder_per_class,\n",
    "                                               class_name + '.csv'\n",
    "                                           )\n",
    "                per_class_df = pd.read_csv(csv_out_path, header=None)\n",
    "                \n",
    "                per_class_df['class_no'] = [class_index]*len(per_class_df)\n",
    "                per_class_df['class_name'] = [class_name]*len(per_class_df)\n",
    "                \n",
    "                per_class_df[per_class_df.columns[0]] = class_name + '/' +  per_class_df[per_class_df.columns[0]]\n",
    "                \n",
    "                if total_df is None:\n",
    "                    total_df = per_class_df\n",
    "                else:\n",
    "                    total_df = pd.concat([total_df, per_class_df], axis=0)\n",
    "            \n",
    "            list_name = [[bodypart.name + '_x', bodypart.name + '_y', \n",
    "                  bodypart.name + '_score'] for bodypart in BodyPart]\n",
    "            \n",
    "            header_name = []\n",
    "            for columns_name in list_name:\n",
    "                header_name += columns_name\n",
    "            header_name = ['filename'] + header_name\n",
    "            header_map = { total_df.columns[i]: header_name[i]\n",
    "                             for i in range(len(header_name))\n",
    "                         }\n",
    "            \n",
    "            total_df.rename(header_map, axis=1, inplace=True)\n",
    "            \n",
    "            return total_df\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "images_in_folder = os.path.join('yoga_poses', 'train')\n",
    "csvs_out_path = 'train_data.csv'\n",
    "train_preprocessor = Preprocessor(\n",
    "    images_in_folder,\n",
    "    csvs_out_path\n",
    ")\n",
    "train_preprocessor.process()   \n",
    "\n",
    "\n",
    "images_in_folder = os.path.join('yoga_poses', 'test')\n",
    "csvs_out_path = 'test_data.csv'\n",
    "test_preprocessor = Preprocessor(\n",
    "    images_in_folder,\n",
    "    csvs_out_path\n",
    ")\n",
    "test_preprocessor.process()\n",
    "            \n",
    "            \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------TRAINING----------------\n",
      "Epoch 1/200\n",
      "31/39 [======================>.......] - ETA: 0s - loss: 1.4786 - accuracy: 0.3710 \n",
      "Epoch 1: val_accuracy improved from -inf to 0.60360, saving model to weights.best.hdf5\n",
      "39/39 [==============================] - 1s 10ms/step - loss: 1.4330 - accuracy: 0.4141 - val_loss: 1.2512 - val_accuracy: 0.6036\n",
      "Epoch 2/200\n",
      "30/39 [======================>.......] - ETA: 0s - loss: 1.1491 - accuracy: 0.5771\n",
      "Epoch 2: val_accuracy did not improve from 0.60360\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 1.1180 - accuracy: 0.5746 - val_loss: 0.9774 - val_accuracy: 0.6036\n",
      "Epoch 3/200\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 0.9386 - accuracy: 0.6250\n",
      "Epoch 3: val_accuracy improved from 0.60360 to 0.72072, saving model to weights.best.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.9261 - accuracy: 0.6276 - val_loss: 0.7754 - val_accuracy: 0.7207\n",
      "Epoch 4/200\n",
      "31/39 [======================>.......] - ETA: 0s - loss: 0.7435 - accuracy: 0.7137\n",
      "Epoch 4: val_accuracy improved from 0.72072 to 0.78378, saving model to weights.best.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.7641 - accuracy: 0.7063 - val_loss: 0.6137 - val_accuracy: 0.7838\n",
      "Epoch 5/200\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 0.6753 - accuracy: 0.7402\n",
      "Epoch 5: val_accuracy improved from 0.78378 to 0.82883, saving model to weights.best.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.6665 - accuracy: 0.7335 - val_loss: 0.5009 - val_accuracy: 0.8288\n",
      "Epoch 6/200\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 0.5842 - accuracy: 0.7773\n",
      "Epoch 6: val_accuracy improved from 0.82883 to 0.88288, saving model to weights.best.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.5749 - accuracy: 0.7833 - val_loss: 0.4319 - val_accuracy: 0.8829\n",
      "Epoch 7/200\n",
      "31/39 [======================>.......] - ETA: 0s - loss: 0.5080 - accuracy: 0.7944\n",
      "Epoch 7: val_accuracy improved from 0.88288 to 0.90991, saving model to weights.best.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.4852 - accuracy: 0.8122 - val_loss: 0.3712 - val_accuracy: 0.9099\n",
      "Epoch 8/200\n",
      "29/39 [=====================>........] - ETA: 0s - loss: 0.4489 - accuracy: 0.8405\n",
      "Epoch 8: val_accuracy did not improve from 0.90991\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.4214 - accuracy: 0.8491 - val_loss: 0.3285 - val_accuracy: 0.9099\n",
      "Epoch 9/200\n",
      "30/39 [======================>.......] - ETA: 0s - loss: 0.4149 - accuracy: 0.8562\n",
      "Epoch 9: val_accuracy did not improve from 0.90991\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.4025 - accuracy: 0.8652 - val_loss: 0.3063 - val_accuracy: 0.9009\n",
      "Epoch 10/200\n",
      "30/39 [======================>.......] - ETA: 0s - loss: 0.3768 - accuracy: 0.8813\n",
      "Epoch 10: val_accuracy improved from 0.90991 to 0.93694, saving model to weights.best.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.3539 - accuracy: 0.8892 - val_loss: 0.2605 - val_accuracy: 0.9369\n",
      "Epoch 11/200\n",
      "31/39 [======================>.......] - ETA: 0s - loss: 0.3143 - accuracy: 0.8992\n",
      "Epoch 11: val_accuracy improved from 0.93694 to 0.94595, saving model to weights.best.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.3197 - accuracy: 0.8957 - val_loss: 0.2426 - val_accuracy: 0.9459\n",
      "Epoch 12/200\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 0.2969 - accuracy: 0.8958\n",
      "Epoch 12: val_accuracy improved from 0.94595 to 0.95495, saving model to weights.best.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.3014 - accuracy: 0.8925 - val_loss: 0.2116 - val_accuracy: 0.9550\n",
      "Epoch 13/200\n",
      "31/39 [======================>.......] - ETA: 0s - loss: 0.2877 - accuracy: 0.9113\n",
      "Epoch 13: val_accuracy improved from 0.95495 to 0.96396, saving model to weights.best.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.2736 - accuracy: 0.9213 - val_loss: 0.1986 - val_accuracy: 0.9640\n",
      "Epoch 14/200\n",
      "30/39 [======================>.......] - ETA: 0s - loss: 0.2323 - accuracy: 0.9333\n",
      "Epoch 14: val_accuracy did not improve from 0.96396\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.2363 - accuracy: 0.9294 - val_loss: 0.1818 - val_accuracy: 0.9550\n",
      "Epoch 15/200\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 0.2223 - accuracy: 0.9223\n",
      "Epoch 15: val_accuracy did not improve from 0.96396\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.2205 - accuracy: 0.9294 - val_loss: 0.1637 - val_accuracy: 0.9369\n",
      "Epoch 16/200\n",
      "29/39 [=====================>........] - ETA: 0s - loss: 0.2137 - accuracy: 0.9267\n",
      "Epoch 16: val_accuracy did not improve from 0.96396\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.2085 - accuracy: 0.9294 - val_loss: 0.1509 - val_accuracy: 0.9369\n",
      "Epoch 17/200\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 0.1924 - accuracy: 0.9453\n",
      "Epoch 17: val_accuracy did not improve from 0.96396\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.1979 - accuracy: 0.9422 - val_loss: 0.1447 - val_accuracy: 0.9459\n",
      "Epoch 18/200\n",
      "30/39 [======================>.......] - ETA: 0s - loss: 0.1723 - accuracy: 0.9563\n",
      "Epoch 18: val_accuracy did not improve from 0.96396\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.1712 - accuracy: 0.9583 - val_loss: 0.1292 - val_accuracy: 0.9640\n",
      "Epoch 19/200\n",
      "29/39 [=====================>........] - ETA: 0s - loss: 0.1748 - accuracy: 0.9461\n",
      "Epoch 19: val_accuracy did not improve from 0.96396\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.1688 - accuracy: 0.9535 - val_loss: 0.1146 - val_accuracy: 0.9459\n",
      "Epoch 20/200\n",
      "29/39 [=====================>........] - ETA: 0s - loss: 0.1766 - accuracy: 0.9526\n",
      "Epoch 20: val_accuracy did not improve from 0.96396\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.1842 - accuracy: 0.9438 - val_loss: 0.1060 - val_accuracy: 0.9640\n",
      "Epoch 21/200\n",
      "30/39 [======================>.......] - ETA: 0s - loss: 0.1557 - accuracy: 0.9563\n",
      "Epoch 21: val_accuracy did not improve from 0.96396\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.1464 - accuracy: 0.9583 - val_loss: 0.1023 - val_accuracy: 0.9640\n",
      "Epoch 22/200\n",
      "27/39 [===================>..........] - ETA: 0s - loss: 0.1275 - accuracy: 0.9653\n",
      "Epoch 22: val_accuracy did not improve from 0.96396\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.1326 - accuracy: 0.9615 - val_loss: 0.1080 - val_accuracy: 0.9550\n",
      "Epoch 23/200\n",
      "27/39 [===================>..........] - ETA: 0s - loss: 0.1062 - accuracy: 0.9745\n",
      "Epoch 23: val_accuracy did not improve from 0.96396\n",
      "39/39 [==============================] - 0s 6ms/step - loss: 0.1223 - accuracy: 0.9663 - val_loss: 0.0778 - val_accuracy: 0.9640\n",
      "Epoch 24/200\n",
      "25/39 [==================>...........] - ETA: 0s - loss: 0.1144 - accuracy: 0.9600\n",
      "Epoch 24: val_accuracy did not improve from 0.96396\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.1208 - accuracy: 0.9599 - val_loss: 0.0796 - val_accuracy: 0.9640\n",
      "Epoch 25/200\n",
      "29/39 [=====================>........] - ETA: 0s - loss: 0.1080 - accuracy: 0.9698\n",
      "Epoch 25: val_accuracy did not improve from 0.96396\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.1051 - accuracy: 0.9711 - val_loss: 0.0738 - val_accuracy: 0.9640\n",
      "Epoch 26/200\n",
      "29/39 [=====================>........] - ETA: 0s - loss: 0.1268 - accuracy: 0.9591\n",
      "Epoch 26: val_accuracy improved from 0.96396 to 0.97297, saving model to weights.best.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.1369 - accuracy: 0.9599 - val_loss: 0.0604 - val_accuracy: 0.9730\n",
      "Epoch 27/200\n",
      "29/39 [=====================>........] - ETA: 0s - loss: 0.1247 - accuracy: 0.9569\n",
      "Epoch 27: val_accuracy did not improve from 0.97297\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.1179 - accuracy: 0.9583 - val_loss: 0.0658 - val_accuracy: 0.9730\n",
      "Epoch 28/200\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 0.1084 - accuracy: 0.9688\n",
      "Epoch 28: val_accuracy did not improve from 0.97297\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.1130 - accuracy: 0.9679 - val_loss: 0.0522 - val_accuracy: 0.9730\n",
      "Epoch 29/200\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 0.1019 - accuracy: 0.9766\n",
      "Epoch 29: val_accuracy improved from 0.97297 to 0.98198, saving model to weights.best.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.1040 - accuracy: 0.9759 - val_loss: 0.0522 - val_accuracy: 0.9820\n",
      "Epoch 30/200\n",
      "30/39 [======================>.......] - ETA: 0s - loss: 0.1131 - accuracy: 0.9625\n",
      "Epoch 30: val_accuracy did not improve from 0.98198\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.1042 - accuracy: 0.9679 - val_loss: 0.0486 - val_accuracy: 0.9820\n",
      "Epoch 31/200\n",
      "28/39 [====================>.........] - ETA: 0s - loss: 0.0929 - accuracy: 0.9643\n",
      "Epoch 31: val_accuracy did not improve from 0.98198\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.0903 - accuracy: 0.9695 - val_loss: 0.0449 - val_accuracy: 0.9820\n",
      "Epoch 32/200\n",
      "30/39 [======================>.......] - ETA: 0s - loss: 0.0903 - accuracy: 0.9771\n",
      "Epoch 32: val_accuracy improved from 0.98198 to 0.99099, saving model to weights.best.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.0960 - accuracy: 0.9759 - val_loss: 0.0365 - val_accuracy: 0.9910\n",
      "Epoch 33/200\n",
      "28/39 [====================>.........] - ETA: 0s - loss: 0.0706 - accuracy: 0.9777\n",
      "Epoch 33: val_accuracy improved from 0.99099 to 1.00000, saving model to weights.best.hdf5\n",
      "39/39 [==============================] - 0s 5ms/step - loss: 0.0766 - accuracy: 0.9775 - val_loss: 0.0311 - val_accuracy: 1.0000\n",
      "Epoch 34/200\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 0.0828 - accuracy: 0.9824\n",
      "Epoch 34: val_accuracy did not improve from 1.00000\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.0771 - accuracy: 0.9839 - val_loss: 0.0418 - val_accuracy: 0.9730\n",
      "Epoch 35/200\n",
      "30/39 [======================>.......] - ETA: 0s - loss: 0.1035 - accuracy: 0.9729\n",
      "Epoch 35: val_accuracy did not improve from 1.00000\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.0895 - accuracy: 0.9775 - val_loss: 0.0335 - val_accuracy: 1.0000\n",
      "Epoch 36/200\n",
      "28/39 [====================>.........] - ETA: 0s - loss: 0.0773 - accuracy: 0.9777\n",
      "Epoch 36: val_accuracy did not improve from 1.00000\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.0763 - accuracy: 0.9759 - val_loss: 0.0327 - val_accuracy: 1.0000\n",
      "Epoch 37/200\n",
      "31/39 [======================>.......] - ETA: 0s - loss: 0.0758 - accuracy: 0.9778\n",
      "Epoch 37: val_accuracy did not improve from 1.00000\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.0749 - accuracy: 0.9791 - val_loss: 0.0381 - val_accuracy: 0.9820\n",
      "Epoch 38/200\n",
      "31/39 [======================>.......] - ETA: 0s - loss: 0.0613 - accuracy: 0.9778\n",
      "Epoch 38: val_accuracy did not improve from 1.00000\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.0593 - accuracy: 0.9823 - val_loss: 0.0359 - val_accuracy: 0.9820\n",
      "Epoch 39/200\n",
      "29/39 [=====================>........] - ETA: 0s - loss: 0.0544 - accuracy: 0.9828\n",
      "Epoch 39: val_accuracy did not improve from 1.00000\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.0545 - accuracy: 0.9856 - val_loss: 0.0222 - val_accuracy: 1.0000\n",
      "Epoch 40/200\n",
      "30/39 [======================>.......] - ETA: 0s - loss: 0.0793 - accuracy: 0.9812\n",
      "Epoch 40: val_accuracy did not improve from 1.00000\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.0755 - accuracy: 0.9823 - val_loss: 0.0306 - val_accuracy: 1.0000\n",
      "Epoch 41/200\n",
      "29/39 [=====================>........] - ETA: 0s - loss: 0.0710 - accuracy: 0.9763\n",
      "Epoch 41: val_accuracy did not improve from 1.00000\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.0620 - accuracy: 0.9823 - val_loss: 0.0392 - val_accuracy: 0.9820\n",
      "Epoch 42/200\n",
      "25/39 [==================>...........] - ETA: 0s - loss: 0.0538 - accuracy: 0.9825\n",
      "Epoch 42: val_accuracy did not improve from 1.00000\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.0559 - accuracy: 0.9823 - val_loss: 0.0299 - val_accuracy: 1.0000\n",
      "Epoch 43/200\n",
      "33/39 [========================>.....] - ETA: 0s - loss: 0.0624 - accuracy: 0.9848\n",
      "Epoch 43: val_accuracy did not improve from 1.00000\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.0677 - accuracy: 0.9807 - val_loss: 0.0292 - val_accuracy: 0.9910\n",
      "Epoch 44/200\n",
      "31/39 [======================>.......] - ETA: 0s - loss: 0.0628 - accuracy: 0.9819\n",
      "Epoch 44: val_accuracy did not improve from 1.00000\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.0749 - accuracy: 0.9807 - val_loss: 0.0305 - val_accuracy: 0.9820\n",
      "Epoch 45/200\n",
      "29/39 [=====================>........] - ETA: 0s - loss: 0.0445 - accuracy: 0.9935\n",
      "Epoch 45: val_accuracy did not improve from 1.00000\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.0519 - accuracy: 0.9904 - val_loss: 0.0188 - val_accuracy: 1.0000\n",
      "Epoch 46/200\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 0.0482 - accuracy: 0.9883\n",
      "Epoch 46: val_accuracy did not improve from 1.00000\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.0499 - accuracy: 0.9888 - val_loss: 0.0225 - val_accuracy: 1.0000\n",
      "Epoch 47/200\n",
      "30/39 [======================>.......] - ETA: 0s - loss: 0.0647 - accuracy: 0.9875\n",
      "Epoch 47: val_accuracy did not improve from 1.00000\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.0572 - accuracy: 0.9872 - val_loss: 0.0279 - val_accuracy: 0.9910\n",
      "Epoch 48/200\n",
      "30/39 [======================>.......] - ETA: 0s - loss: 0.0566 - accuracy: 0.9812\n",
      "Epoch 48: val_accuracy did not improve from 1.00000\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.0491 - accuracy: 0.9856 - val_loss: 0.0144 - val_accuracy: 1.0000\n",
      "Epoch 49/200\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 0.0503 - accuracy: 0.9902\n",
      "Epoch 49: val_accuracy did not improve from 1.00000\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.0556 - accuracy: 0.9856 - val_loss: 0.0186 - val_accuracy: 1.0000\n",
      "Epoch 50/200\n",
      "29/39 [=====================>........] - ETA: 0s - loss: 0.0364 - accuracy: 0.9914\n",
      "Epoch 50: val_accuracy did not improve from 1.00000\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.0367 - accuracy: 0.9904 - val_loss: 0.0199 - val_accuracy: 1.0000\n",
      "Epoch 51/200\n",
      "32/39 [=======================>......] - ETA: 0s - loss: 0.0353 - accuracy: 0.9922\n",
      "Epoch 51: val_accuracy did not improve from 1.00000\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.0386 - accuracy: 0.9904 - val_loss: 0.0151 - val_accuracy: 1.0000\n",
      "Epoch 52/200\n",
      "29/39 [=====================>........] - ETA: 0s - loss: 0.0531 - accuracy: 0.9892\n",
      "Epoch 52: val_accuracy did not improve from 1.00000\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.0482 - accuracy: 0.9888 - val_loss: 0.0154 - val_accuracy: 1.0000\n",
      "Epoch 53/200\n",
      "28/39 [====================>.........] - ETA: 0s - loss: 0.0596 - accuracy: 0.9777\n",
      "Epoch 53: val_accuracy did not improve from 1.00000\n",
      "39/39 [==============================] - 0s 3ms/step - loss: 0.0561 - accuracy: 0.9791 - val_loss: 0.0095 - val_accuracy: 1.0000\n",
      "-----------------EVAUATION----------------\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "LOSS:  0.0021635955199599266\n",
      "ACCURACY:  1.0\n",
      "tfjs model saved at  model\n"
     ]
    }
   ],
   "source": [
    "tfjs_model_dir = 'model'\n",
    "\n",
    "\n",
    "def load_csv(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df.drop(['filename'],axis=1, inplace=True)\n",
    "    classes = df.pop('class_name').unique()\n",
    "    y = df.pop('class_no')\n",
    "    \n",
    "    X = df.astype('float64')\n",
    "    y = keras.utils.to_categorical(y)\n",
    "    \n",
    "    return X, y, classes\n",
    "\n",
    "\n",
    "def get_center_point(landmarks, left_bodypart, right_bodypart):\n",
    "    left = tf.gather(landmarks, left_bodypart.value, axis=1)\n",
    "    right = tf.gather(landmarks, right_bodypart.value, axis=1)\n",
    "    center = left * 0.5 + right * 0.5\n",
    "    return center\n",
    "\n",
    "\n",
    "def get_pose_size(landmarks, torso_size_multiplier=2.5):\n",
    "    \n",
    "    hips_center = get_center_point(landmarks, BodyPart.LEFT_HIP, \n",
    "                                 BodyPart.RIGHT_HIP)\n",
    "\n",
    "    shoulders_center = get_center_point(landmarks, BodyPart.LEFT_SHOULDER,\n",
    "                                      BodyPart.RIGHT_SHOULDER)\n",
    "\n",
    "    torso_size = tf.linalg.norm(shoulders_center - hips_center)\n",
    "    \n",
    "    pose_center_new = get_center_point(landmarks, BodyPart.LEFT_HIP, \n",
    "                                     BodyPart.RIGHT_HIP)\n",
    "    pose_center_new = tf.expand_dims(pose_center_new, axis=1)\n",
    "    \n",
    "    pose_center_new = tf.broadcast_to(pose_center_new,\n",
    "                                    [tf.size(landmarks) // (17*2), 17, 2])\n",
    "\n",
    "    d = tf.gather(landmarks - pose_center_new, 0, axis=0,\n",
    "                name=\"dist_to_pose_center\")\n",
    "   \n",
    "    max_dist = tf.reduce_max(tf.linalg.norm(d, axis=0))\n",
    "\n",
    "    pose_size = tf.maximum(torso_size * torso_size_multiplier, max_dist)\n",
    "    return pose_size\n",
    "\n",
    "\n",
    "\n",
    "def normalize_pose_landmarks(landmarks):\n",
    "    \n",
    "    pose_center = get_center_point(landmarks, BodyPart.LEFT_HIP, \n",
    "                                 BodyPart.RIGHT_HIP)\n",
    "\n",
    "    pose_center = tf.expand_dims(pose_center, axis=1)\n",
    "    \n",
    "    pose_center = tf.broadcast_to(pose_center, \n",
    "                                [tf.size(landmarks) // (17*2), 17, 2])\n",
    "    landmarks = landmarks - pose_center\n",
    "\n",
    "    pose_size = get_pose_size(landmarks)\n",
    "    landmarks /= pose_size\n",
    "    return landmarks\n",
    "\n",
    "\n",
    "def landmarks_to_embedding(landmarks_and_scores):\n",
    "    \n",
    "    reshaped_inputs = keras.layers.Reshape((17, 3))(landmarks_and_scores)\n",
    "\n",
    "    landmarks = normalize_pose_landmarks(reshaped_inputs[:, :, :2])\n",
    "    \n",
    "    embedding = keras.layers.Flatten()(landmarks)\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def preprocess_data(X_train):\n",
    "    processed_X_train = []\n",
    "    for i in range(X_train.shape[0]):\n",
    "        embedding = landmarks_to_embedding(tf.reshape(tf.convert_to_tensor(X_train.iloc[i]), (1, 51)))\n",
    "        processed_X_train.append(tf.reshape(embedding, (34)))\n",
    "    return tf.convert_to_tensor(processed_X_train)\n",
    "\n",
    "\n",
    "X, y, class_names = load_csv(\"train_data.csv\")\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15)\n",
    "X_test, y_test, _ = load_csv('test_data.csv')\n",
    "\n",
    "\n",
    "processed_X_train = preprocess_data(X_train)\n",
    "processed_X_val =  preprocess_data(X_val)\n",
    "processed_X_test = preprocess_data(X_test)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(34))\n",
    "layer = keras.layers.Dense(128, activation=tf.nn.relu6)(inputs)\n",
    "layer = keras.layers.Dropout(0.5)(layer)\n",
    "layer = keras.layers.Dense(64, activation=tf.nn.relu6)(layer)\n",
    "layer = keras.layers.Dropout(0.5)(layer)\n",
    "outputs = keras.layers.Dense(len(class_names), activation=\"softmax\")(layer)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "checkpoint_path = \"weights.best.hdf5\"\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                             monitor='val_accuracy',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             mode='max')\n",
    "earlystopping = keras.callbacks.EarlyStopping(monitor='val_accuracy', \n",
    "                                              patience=20)\n",
    "\n",
    "print('--------------TRAINING----------------')\n",
    "history = model.fit(processed_X_train, y_train,\n",
    "                    epochs=200,\n",
    "                    batch_size=16,\n",
    "                    validation_data=(processed_X_val, y_val),\n",
    "                    callbacks=[checkpoint, earlystopping])\n",
    "\n",
    "\n",
    "print('-----------------EVAUATION----------------')\n",
    "loss, accuracy = model.evaluate(processed_X_test, y_test)\n",
    "print('LOSS: ', loss)\n",
    "print(\"ACCURACY: \", accuracy)\n",
    "\n",
    "\n",
    "tfjs.converters.save_keras_model(model, tfjs_model_dir)\n",
    "print('tfjs model saved at ',tfjs_model_dir)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
